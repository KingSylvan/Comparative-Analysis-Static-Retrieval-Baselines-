# -*- coding: utf-8 -*-
"""Performance Evaluation on MS MARCO Dataset with Comparative Analysis Against Static Retrieval Baselines

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qS4zhDUwKo66Lp7kjmCZ12wclXDPY_l2
"""

# Install all required libraries
!pip install -q sentence-transformers
!pip install -q faiss-cpu
!pip install -q chromadb
!pip install -q openai
!pip install -q datasets
!pip install -q rank-bm25
!pip install -q scikit-learn
!pip install -q psutil
!pip install -q GPUtil
!pip install -q nltk

!pip install -q matplotlib seaborn plotly

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots


plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

import os
import time
import numpy as np
import pandas as pd
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Embedding and vector stores
from sentence_transformers import SentenceTransformer
import faiss
import chromadb

# OpenAI
from openai import OpenAI

# Evaluation
from sklearn.metrics import precision_score, recall_score, f1_score
from datasets import load_dataset

# Resource monitoring
import psutil
import GPUtil

# BM25 for hybrid retrieval
from rank_bm25 import BM25Okapi

# Configure OpenAI API
from google.colab import userdata


try:
    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
except:
    OPENAI_API_KEY = input("Enter OpenAI API key: ")


# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

# Test the API connection
try:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Say 'API connection successful!'"}],
        max_tokens=10
    )
    print(f"OpenAI API Test: {response.choices[0].message.content}")
except Exception as e:
    print(f"API Error: {e}")

print("ðŸ“¥ Loading MS MARCO dataset")

# Load a small subset of MS MARCO for testing
from datasets import load_dataset

# Load just 100 passages for initial testing
dataset = load_dataset("ms_marco", "v1.1", split="train", streaming=True)

# Convert to list (take first 100 samples)
sample_data = []
for i, item in enumerate(dataset):
    if i >= 100:  # Start with 100 documents
        break
    if 'passages' in item and item['passages']['passage_text']:
        # Extract the first passage text
        passage = item['passages']['passage_text'][0]
        sample_data.append({
            'id': i,
            'text': passage,
            'query': item.get('query', '')
        })

print(f"Loaded {len(sample_data)} documents")
print(f"\n Sample document:")
print(f"Query: {sample_data[0]['query']}")
print(f"Text: {sample_data[0]['text'][:200]}")
print(f"\n Average text length: {np.mean([len(d['text']) for d in sample_data]):.0f} characters")

# Alternative Simple custom corpus for testing
print("Creating custom test corpus")

sample_data = [
    {
        'id': 0,
        'text': 'Python is a high-level programming language known for its simplicity and readability. It supports multiple programming paradigms including procedural, object-oriented, and functional programming.',
        'query': 'What is Python programming language?'
    },
    {
        'id': 1,
        'text': 'Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.',
        'query': 'What is machine learning?'
    },
    {
        'id': 2,
        'text': 'Neural networks are computing systems inspired by biological neural networks that constitute animal brains. They consist of interconnected nodes or neurons that process information using a connectionist approach to computation.',
        'query': 'What are neural networks?'
    },
    {
        'id': 3,
        'text': 'Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics.',
        'query': 'What is NLP?'
    },
    {
        'id': 4,
        'text': 'Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.',
        'query': 'What is deep learning?'
    },
    {
        'id': 5,
        'text': 'FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.',
        'query': 'What is FAISS?'
    },
    {
        'id': 6,
        'text': 'Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them to generate more accurate and contextual responses.',
        'query': 'What is RAG?'
    },
    {
        'id': 7,
        'text': 'Vector embeddings are numerical representations of data in a high-dimensional space. They capture semantic meaning and relationships between different pieces of text, enabling similarity searches and various machine learning tasks.',
        'query': 'What are vector embeddings?'
    },
    {
        'id': 8,
        'text': 'Transformers are a type of deep learning model architecture introduced in 2017. They rely on self-attention mechanisms and have become the foundation for modern language models like BERT, GPT, and T5.',
        'query': 'What are transformers in AI?'
    },
    {
        'id': 9,
        'text': 'BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that understands context by looking at surrounding text from both directions. It has revolutionized many NLP tasks.',
        'query': 'What is BERT?'
    }
]

print(f"Created {len(sample_data)} documents")
print(f"\n Sample document:")
print(f"Query: {sample_data[0]['query']}")
print(f"Text: {sample_data[0]['text']}")
print(f"\n Average text length: {np.mean([len(d['text']) for d in sample_data]):.0f} characters")

print("Loading embedding models")
print("This may take 1-2 minutes on first run (models are ~80MB each)\n")

start_time = time.time()

# Load all-MiniLM-L6-v2
print("Loading all-MiniLM-L6-v2")
model_minilm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
print(f"Loaded | Embedding dim: {model_minilm.get_sentence_embedding_dimension()}")

# Load bge-small
print("Loading bge-small-en-v1.5")
model_bge = SentenceTransformer('BAAI/bge-small-en-v1.5')
print(f"Loaded | Embedding dim: {model_bge.get_sentence_embedding_dimension()}")

load_time = time.time() - start_time
print(f"\n Total loading time: {load_time:.2f} seconds")

# Test embeddings
test_text = "This is a test sentence"
test_emb_minilm = model_minilm.encode(test_text)
test_emb_bge = model_bge.encode(test_text)

print(f"\n Test embeddings:")
print(f"   MiniLM shape: {test_emb_minilm.shape}")
print(f"   BGE shape: {test_emb_bge.shape}")
print(f"   MiniLM sample: {test_emb_minilm[:5]}")
print(f"   BGE sample: {test_emb_bge[:5]}")

print("Creating embeddings for MS MARCO corpus (100 docs)\n")

# Extract texts from sample_data
corpus_texts = [doc['text'] for doc in sample_data]
corpus_ids = [doc['id'] for doc in sample_data]

# Time and create embeddings with MiniLM
print("Encoding with all-MiniLM-L6-v2")
start_time = time.time()
embeddings_minilm = model_minilm.encode(
    corpus_texts,
    show_progress_bar=True,
    convert_to_numpy=True
)
minilm_time = time.time() - start_time
print(f" Done in {minilm_time:.2f}s | Shape: {embeddings_minilm.shape}")

# Time and create embeddings with BGE
print("\n Encoding with bge-small-en-v1.5")
start_time = time.time()
embeddings_bge = model_bge.encode(
    corpus_texts,
    show_progress_bar=True,
    convert_to_numpy=True
)
bge_time = time.time() - start_time
print(f"  Done in {bge_time:.2f}s | Shape: {embeddings_bge.shape}")

# Summary
print(f"\n Embedding Creation Summary:")
print(f"   Corpus size: {len(corpus_texts)} documents")
print(f"   MiniLM: {minilm_time:.2f}s ({len(corpus_texts)/minilm_time:.1f} docs/sec)")
print(f"   BGE: {bge_time:.2f}s ({len(corpus_texts)/bge_time:.1f} docs/sec)")
print(f"   Speed ratio: BGE is {minilm_time/bge_time:.2f}x the speed of MiniLM")

import sys

print(" Memory Usage Analysis:\n")

# Calculate embedding memory size
minilm_size_mb = embeddings_minilm.nbytes / (1024**2)
bge_size_mb = embeddings_bge.nbytes / (1024**2)

print(f"MiniLM embeddings: {minilm_size_mb:.2f} MB")
print(f"BGE embeddings: {bge_size_mb:.2f} MB")

# System memory
memory_info = psutil.virtual_memory()
print(f"\nSystem RAM:")
print(f"   Total: {memory_info.total / (1024**3):.2f} GB")
print(f"   Available: {memory_info.available / (1024**3):.2f} GB")
print(f"   Used: {memory_info.percent}%")

print(" Creating FAISS indexes\n")

# FAISS with MiniLM embeddings
print("Building FAISS index with MiniLM embeddings")
start_time = time.time()

dimension_minilm = embeddings_minilm.shape[1]
index_faiss_minilm = faiss.IndexFlatL2(dimension_minilm)
index_faiss_minilm.add(embeddings_minilm)

faiss_minilm_time = time.time() - start_time
print(f"   Built in {faiss_minilm_time:.4f}s")
print(f"   Index size: {index_faiss_minilm.ntotal} vectors")

# FAISS with BGE embeddings
print("\n Building FAISS index with BGE embeddings")
start_time = time.time()

dimension_bge = embeddings_bge.shape[1]
index_faiss_bge = faiss.IndexFlatL2(dimension_bge)
index_faiss_bge.add(embeddings_bge)

faiss_bge_time = time.time() - start_time
print(f"   Built in {faiss_bge_time:.4f}s")
print(f"   Index size: {index_faiss_bge.ntotal} vectors")

print(f"\n FAISS Indexing Summary:")
print(f"   MiniLM: {faiss_minilm_time*1000:.2f}ms")
print(f"   BGE: {faiss_bge_time*1000:.2f}ms")

print("Creating Chroma indexes\n")

# Initialize Chroma client
chroma_client = chromadb.Client()

# Chroma with MiniLM embeddings
print("Building Chroma collection with MiniLM embeddings")
start_time = time.time()

# Delete collection if exists
try:
    chroma_client.delete_collection("minilm_collection")
except:
    pass

collection_minilm = chroma_client.create_collection(
    name="minilm_collection",
    metadata={"hnsw:space": "cosine"}
)

# Add documents
collection_minilm.add(
    embeddings=embeddings_minilm.tolist(),
    documents=corpus_texts,
    ids=[str(i) for i in corpus_ids]
)

chroma_minilm_time = time.time() - start_time
print(f"  Built in {chroma_minilm_time:.4f}s")
print(f"   Collection size: {collection_minilm.count()} documents")

# Chroma with BGE embeddings
print("\n Building Chroma collection with BGE embeddings")
start_time = time.time()

# Delete collection if exists
try:
    chroma_client.delete_collection("bge_collection")
except:
    pass

collection_bge = chroma_client.create_collection(
    name="bge_collection",
    metadata={"hnsw:space": "cosine"}
)

# Add documents
collection_bge.add(
    embeddings=embeddings_bge.tolist(),
    documents=corpus_texts,
    ids=[str(i) for i in corpus_ids]
)

chroma_bge_time = time.time() - start_time
print(f"  Built in {chroma_bge_time:.4f}s")
print(f"   Collection size: {collection_bge.count()} documents")

print(f"\n Chroma Indexing Summary:")
print(f"   MiniLM: {chroma_minilm_time*1000:.2f}ms")
print(f"   BGE: {chroma_bge_time*1000:.2f}ms")

print(" Vector Store Comparison:\n")

# Create comparison dataframe
comparison_df = pd.DataFrame({
    'Vector Store': ['FAISS + MiniLM', 'FAISS + BGE', 'Chroma + MiniLM', 'Chroma + BGE'],
    'Indexing Time (ms)': [
        faiss_minilm_time * 1000,
        faiss_bge_time * 1000,
        chroma_minilm_time * 1000,
        chroma_bge_time * 1000
    ],
    'Num Vectors': [
        index_faiss_minilm.ntotal,
        index_faiss_bge.ntotal,
        collection_minilm.count(),
        collection_bge.count()
    ]
})

print(comparison_df.to_string(index=False))

# Find fastest
fastest = comparison_df.loc[comparison_df['Indexing Time (ms)'].idxmin()]
print(f"\nFastest indexing: {fastest['Vector Store']} ({fastest['Indexing Time (ms)']:.2f}ms)")

print("reparing test queries\n")

# Create test queries based on our corpus
test_queries = [
    "What is Python programming language?",
    "Explain machine learning",
    "How do neural networks work?",
    "What is natural language processing?",
    "Tell me about deep learning"
]

print(f" Created {len(test_queries)} test queries:")
for  q in enumerate(test_queries, 1):
    print(f"   {i}. {q}")

# Generate query embeddings
print("\n Generating query embeddings")

query_emb_minilm = model_minilm.encode(test_queries, convert_to_numpy=True)
query_emb_bge = model_bge.encode(test_queries, convert_to_numpy=True)

print(f"  MiniLM query embeddings: {query_emb_minilm.shape}")
print(f"  BGE query embeddings: {query_emb_bge.shape}")

print("ðŸ” Testing FAISS retrieval performance\n")

k = 3  # Retrieve top 3 documents

# Test FAISS + MiniLM
print(" FAISS + MiniLM:")
retrieval_times_faiss_minilm = []

for i, query in enumerate(test_queries):
    start_time = time.time()

    # Search
    distances, indices = index_faiss_minilm.search(
        query_emb_minilm[i:i+1], k
    )

    latency = (time.time() - start_time) * 1000  # Convert to ms
    retrieval_times_faiss_minilm.append(latency)

    print(f"\n   Query {i+1}: '{query}'")
    print(f"    Latency: {latency:.3f}ms")
    print(f"    Top result: {corpus_texts[indices[0][0]][:80]}")
    print(f"    Distance: {distances[0][0]:.4f}")

avg_faiss_minilm = np.mean(retrieval_times_faiss_minilm)
print(f"\n   Average latency: {avg_faiss_minilm:.3f}ms")

# Test FAISS + BGE
print("\n" + "="*70)
print(" FAISS + BGE:")
retrieval_times_faiss_bge = []

for i, query in enumerate(test_queries):
    start_time = time.time()

    # Search
    distances, indices = index_faiss_bge.search(
        query_emb_bge[i:i+1], k
    )

    latency = (time.time() - start_time) * 1000
    retrieval_times_faiss_bge.append(latency)

    print(f"\n   Query {i+1}: '{query}'")
    print(f"    Latency: {latency:.3f}ms")
    print(f"    Top result: {corpus_texts[indices[0][0]][:80]}")
    print(f"    Distance: {distances[0][0]:.4f}")

avg_faiss_bge = np.mean(retrieval_times_faiss_bge)
print(f"\n   Average latency: {avg_faiss_bge:.3f}ms")

print("ðŸ” Testing Chroma retrieval performance\n")

k = 3  # Retrieve top 3 documents

# Test Chroma + MiniLM
print(" Chroma + MiniLM:")
retrieval_times_chroma_minilm = []

for i, query in enumerate(test_queries):
    start_time = time.time()

    # Search
    results = collection_minilm.query(
        query_embeddings=query_emb_minilm[i:i+1].tolist(),
        n_results=k
    )

    latency = (time.time() - start_time) * 1000
    retrieval_times_chroma_minilm.append(latency)

    print(f"\n   Query {i+1}: '{query}'")
    print(f"    Latency: {latency:.3f}ms")
    print(f"    Top result: {results['documents'][0][0][:80]}")
    print(f"    Distance: {results['distances'][0][0]:.4f}")

avg_chroma_minilm = np.mean(retrieval_times_chroma_minilm)
print(f"\n   Average latency: {avg_chroma_minilm:.3f}ms")

# Test Chroma + BGE
print("\n" + "="*70)
print(" Chroma + BGE:")
retrieval_times_chroma_bge = []

for i, query in enumerate(test_queries):
    start_time = time.time()

    # Search
    results = collection_bge.query(
        query_embeddings=query_emb_bge[i:i+1].tolist(),
        n_results=k
    )

    latency = (time.time() - start_time) * 1000
    retrieval_times_chroma_bge.append(latency)

    print(f"\n   Query {i+1}: '{query}'")
    print(f"    Latency: {latency:.3f}ms")
    print(f"    Top result: {results['documents'][0][0][:80]}")
    print(f"    Distance: {results['distances'][0][0]:.4f}")

avg_chroma_bge = np.mean(retrieval_times_chroma_bge)
print(f"\n   Average latency: {avg_chroma_bge:.3f}ms")

def rag_pipeline(query, vector_store, embedding_model, k=3):
    """
    Complete RAG pipeline: Query -> Retrieve -> Generate

    Returns: response, latency_breakdown, retrieved_docs
    """
    times = {}

    # 1. Query Embedding
    start = time.time()
    if embedding_model == 'minilm':
        query_emb = model_minilm.encode([query], convert_to_numpy=True)
    else:  # bge
        query_emb = model_bge.encode([query], convert_to_numpy=True)
    times['embedding'] = (time.time() - start) * 1000

    # 2. Retrieval
    start = time.time()
    if vector_store == 'faiss':
        if embedding_model == 'minilm':
            distances, indices = index_faiss_minilm.search(query_emb, k)
            retrieved_docs = [corpus_texts[idx] for idx in indices[0]]
        else:
            distances, indices = index_faiss_bge.search(query_emb, k)
            retrieved_docs = [corpus_texts[idx] for idx in indices[0]]
    else:  # chroma
        if embedding_model == 'minilm':
            results = collection_minilm.query(
                query_embeddings=query_emb.tolist(),
                n_results=k
            )
        else:
            results = collection_bge.query(
                query_embeddings=query_emb.tolist(),
                n_results=k
            )
        retrieved_docs = results['documents'][0]

    times['retrieval'] = (time.time() - start) * 1000

    # 3. Generate Response with OpenAI
    start = time.time()
    context = "\n\n".join([f"Document {i+1}: {doc}" for i, doc in enumerate(retrieved_docs)])

    prompt = f"""Based on the following context, answer the question accurately and concisely.

Context:
{context}

Question: {query}

Answer:"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150,
        temperature=0.7
    )

    times['generation'] = (time.time() - start) * 1000
    times['total'] = sum(times.values())

    answer = response.choices[0].message.content
    tokens_used = response.usage.total_tokens

    return answer, times, retrieved_docs, tokens_used

print(" RAG pipeline function created!")

print(" Testing all 4 RAG combinations\n")
print("="*80)

# Test query
test_query = "What is Python programming language?"

configurations = [
    ('faiss', 'minilm', 'FAISS + MiniLM'),
    ('faiss', 'bge', 'FAISS + BGE'),
    ('chroma', 'minilm', 'Chroma + MiniLM'),
    ('chroma', 'bge', 'Chroma + BGE'),
]

results_summary = []

for vector_store, embedding_model, name in configurations:
    print(f"\n Testing: {name}")
    print("-" * 80)

    answer, times, docs, tokens = rag_pipeline(
        test_query,
        vector_store,
        embedding_model
    )

    print(f"\nâ“ Query: {test_query}")
    print(f"\n Retrieved Documents:")
    for i, doc in enumerate(docs, 1):
        print(f"   {i}. {doc[:100]}")

    print(f"\nðŸ¤– Generated Answer:")
    print(f"   {answer}")

    print(f"\n Performance Breakdown:")
    print(f"   Query Embedding: {times['embedding']:.2f}ms")
    print(f"   Retrieval: {times['retrieval']:.2f}ms")
    print(f"   Response Generation: {times['generation']:.2f}ms")
    print(f"    TOTAL LATENCY: {times['total']:.2f}ms")
    print(f"   ðŸ’° Tokens Used: {tokens}")

    results_summary.append({
        'Configuration': name,
        'Embedding (ms)': times['embedding'],
        'Retrieval (ms)': times['retrieval'],
        'Generation (ms)': times['generation'],
        'Total (ms)': times['total'],
        'Tokens': tokens
    })

    print("\n" + "="*80)

# Create summary table
print("\n PERFORMANCE COMPARISON TABLE:\n")
summary_df = pd.DataFrame(results_summary)
print(summary_df.to_string(index=False))

# Find best performer
fastest = summary_df.loc[summary_df['Total (ms)'].idxmin()]
print(f"\nFASTEST CONFIGURATION: {fastest['Configuration']}")
print(f"   Total Latency: {fastest['Total (ms)']:.2f}ms")



print("ðŸ“¥ Loading larger MS MARCO dataset for evaluation\n")

from datasets import load_dataset
import random

# Load MS MARCO queries with relevance judgments
print("Loading MS MARCO passages and queries")
start_time = time.time()

# Load dataset
dataset = load_dataset("ms_marco", "v1.1", split="train", streaming=True)

# Collect 500 passages with queries
corpus_large = []
query_doc_pairs = []  # For ground truth evaluation

for i, item in enumerate(dataset):
    if i >= 500:
        break

    if 'passages' in item and item['passages']['passage_text']:
        passage = item['passages']['passage_text'][0]
        query = item.get('query', '')

        # Store passage
        corpus_large.append({
            'id': i,
            'text': passage,
            'query': query
        })

        # Store query-doc pair for evaluation
        if query and len(query) > 10:  # Only meaningful queries
            query_doc_pairs.append({
                'query': query,
                'relevant_doc_id': i,
                'relevant_text': passage
            })

    if i % 100 == 0:
        print(f"   Processed {i} documents")

load_time = time.time() - start_time

print(f"\n Loaded {len(corpus_large)} documents")
print(f" Created {len(query_doc_pairs)} query-document pairs")
print(f" Loading time: {load_time:.2f}s")

# Extract texts and IDs
corpus_texts_large = [doc['text'] for doc in corpus_large]
corpus_ids_large = [doc['id'] for doc in corpus_large]

print(f"\n Dataset Statistics:")
print(f"   Documents: {len(corpus_texts_large)}")
print(f"   Avg length: {np.mean([len(t) for t in corpus_texts_large]):.0f} chars")
print(f"   Min length: {min([len(t) for t in corpus_texts_large])} chars")
print(f"   Max length: {max([len(t) for t in corpus_texts_large])} chars")

# Sample some queries for testing
print(f"\n Sample Queries:")
sample_queries = random.sample(query_doc_pairs, min(5, len(query_doc_pairs)))
for i, pair in enumerate(sample_queries, 1):
    print(f"   {i}. {pair['query']}")

print(" Creating embeddings for 500-document corpus\n")
print("This will take ~30-60 seconds")

# MiniLM embeddings
print(" Encoding with all-MiniLM-L6-v2")
start_time = time.time()
embeddings_minilm_large = model_minilm.encode(
    corpus_texts_large,
    show_progress_bar=True,
    convert_to_numpy=True,
    batch_size=32
)
minilm_time = time.time() - start_time
print(f"  Done in {minilm_time:.2f}s | Shape: {embeddings_minilm_large.shape}")
print(f"   Speed: {len(corpus_texts_large)/minilm_time:.1f} docs/sec")

# BGE embeddings
print("\n Encoding with bge-small-en-v1.5")
start_time = time.time()
embeddings_bge_large = model_bge.encode(
    corpus_texts_large,
    show_progress_bar=True,
    convert_to_numpy=True,
    batch_size=32
)
bge_time = time.time() - start_time
print(f"  Done in {bge_time:.2f}s | Shape: {embeddings_bge_large.shape}")
print(f"   Speed: {len(corpus_texts_large)/bge_time:.1f} docs/sec")

# Memory usage
minilm_mem = embeddings_minilm_large.nbytes / (1024**2)
bge_mem = embeddings_bge_large.nbytes / (1024**2)

print(f"\nðŸ’¾ Memory Usage:")
print(f"   MiniLM: {minilm_mem:.2f} MB")
print(f"   BGE: {bge_mem:.2f} MB")
print(f"   Total: {minilm_mem + bge_mem:.2f} MB")

print(" Building vector indexes for large corpus\n")

# FAISS indexes
print(" Building FAISS indexes")

# MiniLM
start_time = time.time()
index_faiss_minilm_large = faiss.IndexFlatL2(embeddings_minilm_large.shape[1])
index_faiss_minilm_large.add(embeddings_minilm_large)
faiss_minilm_time = time.time() - start_time
print(f"  FAISS + MiniLM: {faiss_minilm_time*1000:.2f}ms | {index_faiss_minilm_large.ntotal} vectors")

# BGE
start_time = time.time()
index_faiss_bge_large = faiss.IndexFlatL2(embeddings_bge_large.shape[1])
index_faiss_bge_large.add(embeddings_bge_large)
faiss_bge_time = time.time() - start_time
print(f"  FAISS + BGE: {faiss_bge_time*1000:.2f}ms | {index_faiss_bge_large.ntotal} vectors")

# Chroma collections
print("\n Building Chroma collections")

# Delete old collections
for coll_name in ["minilm_large", "bge_large"]:
    try:
        chroma_client.delete_collection(coll_name)
    except:
        pass

# MiniLM
start_time = time.time()
collection_minilm_large = chroma_client.create_collection(
    name="minilm_large",
    metadata={"hnsw:space": "cosine"}
)
collection_minilm_large.add(
    embeddings=embeddings_minilm_large.tolist(),
    documents=corpus_texts_large,
    ids=[str(i) for i in corpus_ids_large]
)
chroma_minilm_time = time.time() - start_time
print(f"  Chroma + MiniLM: {chroma_minilm_time:.2f}s | {collection_minilm_large.count()} docs")

# BGE
start_time = time.time()
collection_bge_large = chroma_client.create_collection(
    name="bge_large",
    metadata={"hnsw:space": "cosine"}
)
collection_bge_large.add(
    embeddings=embeddings_bge_large.tolist(),
    documents=corpus_texts_large,
    ids=[str(i) for i in corpus_ids_large]
)
chroma_bge_time = time.time() - start_time
print(f"  Chroma + BGE: {chroma_bge_time:.2f}s | {collection_bge_large.count()} docs")

print(f"\n All indexes built successfully!")

print("ðŸ” Building BM25 index for sparse retrieval\n")

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download required NLTK data
print("Downloading NLTK data")
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('punkt_tab', quiet=True)

# Tokenize corpus
print("\n Tokenizing corpus for BM25")
stop_words = set(stopwords.words('english'))

def tokenize_text(text):
    """Simple tokenization with stopword removal"""
    tokens = word_tokenize(text.lower())
    return [t for t in tokens if t.isalnum() and t not in stop_words]

start_time = time.time()
tokenized_corpus = [tokenize_text(doc) for doc in corpus_texts_large]
tokenize_time = time.time() - start_time

print(f"  Tokenized {len(tokenized_corpus)} documents in {tokenize_time:.2f}s")
print(f"   Avg tokens per doc: {np.mean([len(doc) for doc in tokenized_corpus]):.0f}")

# Build BM25 index
print("\n Building BM25 index")
start_time = time.time()
bm25 = BM25Okapi(tokenized_corpus)
bm25_time = time.time() - start_time

print(f"  BM25 index built in {bm25_time:.2f}s")

# Test BM25 retrieval
test_query = "what is python programming"
tokenized_query = tokenize_text(test_query)
bm25_scores = bm25.get_scores(tokenized_query)
top_indices = np.argsort(bm25_scores)[::-1][:3]

print(f"\n Test BM25 retrieval:")
print(f"   Query: '{test_query}'")
print(f"   Top 3 results:")
for i, idx in enumerate(top_indices, 1):
    print(f"      {i}. Score: {bm25_scores[idx]:.2f} | {corpus_texts_large[idx][:80]}")

print("\n BM25 sparse retrieval ready!")

print(" Generating Dataset Analysis Visualizations\n")

# Create figure directory
import os
os.makedirs('/content/figures', exist_ok=True)

# ============================================================================
# FIGURE 1: Dataset Statistics

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Dataset Analysis: MS MARCO Corpus Statistics', fontsize=16, fontweight='bold')

# 1. Document Length Distribution
doc_lengths = [len(text) for text in corpus_texts_large]
axes[0, 0].hist(doc_lengths, bins=30, color='steelblue', edgecolor='black', alpha=0.7)
axes[0, 0].axvline(np.mean(doc_lengths), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(doc_lengths):.0f}')
axes[0, 0].set_xlabel('Document Length (characters)', fontsize=11)
axes[0, 0].set_ylabel('Frequency', fontsize=11)
axes[0, 0].set_title('(a) Document Length Distribution', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. Query Length Distribution
query_lengths = [len(pair['query'].split()) for pair in query_doc_pairs[:50]]
axes[0, 1].hist(query_lengths, bins=15, color='darkorange', edgecolor='black', alpha=0.7)
axes[0, 1].axvline(np.mean(query_lengths), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(query_lengths):.1f}')
axes[0, 1].set_xlabel('Query Length (words)', fontsize=11)
axes[0, 1].set_ylabel('Frequency', fontsize=11)
axes[0, 1].set_title('(b) Query Length Distribution', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. Query Complexity Distribution
if 'Complexity_Dist' in all_results['Adaptive_Hybrid']:
    complexity_dist = all_results['Adaptive_Hybrid']['Complexity_Dist']
    labels = list(complexity_dist.keys())
    sizes = list(complexity_dist.values())
    colors = ['#66b3ff', '#ff9999', '#99ff99']

    axes[1, 0].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    axes[1, 0].set_title('(c) Query Complexity Classification', fontsize=12, fontweight='bold')

# 4. Embedding Dimension Comparison
embedding_info = {
    'Model': ['MiniLM-L6-v2', 'BGE-Small'],
    'Dimensions': [384, 384],
    'Size (MB)': [80, 85]
}
x = np.arange(len(embedding_info['Model']))
width = 0.35

axes[1, 1].bar(x - width/2, embedding_info['Dimensions'], width, label='Dimensions', color='skyblue')
axes[1, 1].bar(x + width/2, [s*4 for s in embedding_info['Size (MB)']], width, label='Size (MB Ã— 4)', color='lightcoral')
axes[1, 1].set_xlabel('Embedding Model', fontsize=11)
axes[1, 1].set_ylabel('Value', fontsize=11)
axes[1, 1].set_title('(d) Embedding Model Properties', fontsize=12, fontweight='bold')
axes[1, 1].set_xticks(x)
axes[1, 1].set_xticklabels(embedding_info['Model'])
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('/content/figures/fig1_dataset_analysis.png', dpi=300, bbox_inches='tight')
print(" Figure 1: Dataset Analysis saved to /content/figures/fig1_dataset_analysis.png")
plt.show()







print("uilding Query Complexity Classifier (NOVEL COMPONENT)\n")

import re

class QueryComplexityClassifier:
    """
    Classifies queries into complexity levels to enable adaptive retrieval.

    Novel Contribution: Lightweight feature-based classifier that enables
    dynamic retrieval strategy selection without heavy ML models.
    """

    def __init__(self):
        # Question type keywords
        self.factual_keywords = ['what', 'who', 'when', 'where', 'which']
        self.reasoning_keywords = ['why', 'how', 'explain', 'compare', 'difference', 'analyze']
        self.multi_hop_indicators = ['and', 'also', 'additionally', 'furthermore', 'both']

    def extract_features(self, query):
        """Extract features from query"""
        query_lower = query.lower()
        tokens = query_lower.split()

        features = {
            'length': len(tokens),
            'char_length': len(query),
            'has_question_mark': '?' in query,
            'is_factual': any(kw in query_lower for kw in self.factual_keywords),
            'is_reasoning': any(kw in query_lower for kw in self.reasoning_keywords),
            'is_multi_hop': any(kw in query_lower for kw in self.multi_hop_indicators),
            'has_numbers': bool(re.search(r'\d', query)),
            'num_entities': len([t for t in tokens if t[0].isupper()]),  # Rough entity count
        }

        return features

    def classify(self, query):
        """
        Classify query complexity into: SIMPLE, MODERATE, or COMPLEX

        Returns: complexity_level, confidence_score, features
        """
        features = self.extract_features(query)

        # Scoring logic
        complexity_score = 0

        # Length-based scoring
        if features['length'] > 10:
            complexity_score += 2
        elif features['length'] > 6:
            complexity_score += 1

        # Type-based scoring
        if features['is_reasoning']:
            complexity_score += 2
        if features['is_multi_hop']:
            complexity_score += 2
        if features['is_factual'] and not features['is_reasoning']:
            complexity_score -= 1  # Factual questions are simpler

        # Classify based on score
        if complexity_score <= 1:
            level = 'SIMPLE'
            confidence = 0.8
        elif complexity_score <= 3:
            level = 'MODERATE'
            confidence = 0.7
        else:
            level = 'COMPLEX'
            confidence = 0.75

        return level, confidence, features

# Initialize classifier
classifier = QueryComplexityClassifier()

print(" Query Complexity Classifier initialized!\n")

# Test on sample queries
print(" Testing classifier on sample queries:\n")
test_queries = [
    "what is python",
    "what animal is a possum",
    "how does machine learning work",
    "compare supervised and unsupervised learning and explain the differences",
    "why is deep learning better than traditional ML and when should I use it"
]

print("="*80)
for query in test_queries:
    level, confidence, features = classifier.classify(query)
    print(f"\n Query: '{query}'")
    print(f"    Complexity: {level} (confidence: {confidence:.2f})")
    print(f"    Features: length={features['length']}, "
          f"factual={features['is_factual']}, "
          f"reasoning={features['is_reasoning']}, "
          f"multi_hop={features['is_multi_hop']}")
    print("-"*80)

class AdaptiveHybridRetriever:
    """
    Novel adaptive retrieval system that dynamically selects and weights
    retrieval strategies based on query complexity.

    Key Innovation: Unlike static RAG systems, this adapts retrieval strategy
    per-query based on complexity classification.
    """

    def __init__(self, classifier, bm25_index, tokenized_corpus,
                 dense_index, embedding_model, corpus_texts):
        self.classifier = classifier
        self.bm25 = bm25_index
        self.tokenized_corpus = tokenized_corpus
        self.dense_index = dense_index
        self.embedding_model = embedding_model
        self.corpus_texts = corpus_texts

    def retrieve(self, query, k=5, return_scores=False):
        """
        Adaptive retrieval with dynamic strategy selection

        Returns: retrieved_docs, retrieval_metadata
        """
        metadata = {}
        start_time = time.time()

        # 1. Classify query complexity
        complexity, confidence, features = self.classifier.classify(query)
        metadata['complexity'] = complexity
        metadata['confidence'] = confidence
        metadata['features'] = features

        # 2. Determine retrieval strategy based on complexity
        if complexity == 'SIMPLE':
            # Simple queries: Use fast sparse retrieval (BM25)
            strategy = 'sparse_only'
            alpha = 1.0  # 100% sparse
        elif complexity == 'MODERATE':
            # Moderate queries: Hybrid with sparse preference
            strategy = 'hybrid_sparse_heavy'
            alpha = 0.7  # 70% sparse, 30% dense
        else:  # COMPLEX
            # Complex queries: Hybrid with dense preference
            strategy = 'hybrid_dense_heavy'
            alpha = 0.3  # 30% sparse, 70% dense

        metadata['strategy'] = strategy
        metadata['alpha'] = alpha

        # 3. BM25 sparse retrieval
        sparse_start = time.time()
        tokenized_query = tokenize_text(query)
        bm25_scores = self.bm25.get_scores(tokenized_query)
        metadata['sparse_time'] = (time.time() - sparse_start) * 1000

        # 4. Dense retrieval
        dense_start = time.time()
        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)

        if 'faiss' in str(type(self.dense_index)):
            # FAISS
            distances, indices = self.dense_index.search(query_embedding, len(self.corpus_texts))
            # Convert distances to similarity scores (smaller distance = higher similarity)
            dense_scores = 1 / (1 + distances[0])
        else:
            # Chroma (not used in this version but keeping for compatibility)
            results = self.dense_index.query(
                query_embeddings=query_embedding.tolist(),
                n_results=len(self.corpus_texts)
            )
            dense_scores = np.array([1 - d for d in results['distances'][0]])
            indices = np.array([int(id) for id in results['ids'][0]])

        metadata['dense_time'] = (time.time() - dense_start) * 1000

        # 5. Normalize scores to [0, 1]
        bm25_scores_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)
        dense_scores_norm = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-10)

        # 6. Hybrid scoring with adaptive weighting
        hybrid_scores = alpha * bm25_scores_norm + (1 - alpha) * dense_scores_norm

        # 7. Get top-k results
        top_indices = np.argsort(hybrid_scores)[::-1][:k]
        retrieved_docs = [self.corpus_texts[idx] for idx in top_indices]

        metadata['total_time'] = (time.time() - start_time) * 1000
        metadata['top_indices'] = top_indices.tolist()

        if return_scores:
            metadata['scores'] = {
                'bm25': bm25_scores_norm[top_indices].tolist(),
                'dense': dense_scores_norm[top_indices].tolist(),
                'hybrid': hybrid_scores[top_indices].tolist()
            }

        return retrieved_docs, metadata

# Initialize adaptive retriever with FAISS + BGE (best performer from Phase 6)
adaptive_retriever = AdaptiveHybridRetriever(
    classifier=classifier,
    bm25_index=bm25,
    tokenized_corpus=tokenized_corpus,
    dense_index=index_faiss_bge_large,
    embedding_model=model_bge,
    corpus_texts=corpus_texts_large
)

print(" Adaptive Hybrid Retriever initialized!\n")

# Test on queries with different complexities
print(" Testing Adaptive Hybrid Retrieval:\n")
print("="*80)

test_queries_adaptive = [
    "what is rba",  # SIMPLE
    "how can i get a job as a food tester",  # MODERATE
    "compare machine learning and deep learning"  # COMPLEX (if in corpus)
]

for query in test_queries_adaptive:
    print(f"\n Query: '{query}'")
    docs, metadata = adaptive_retriever.retrieve(query, k=3, return_scores=True)

    print(f"\n    Complexity: {metadata['complexity']} (confidence: {metadata['confidence']:.2f})")
    print(f"    Strategy: {metadata['strategy']} (Î±={metadata['alpha']:.2f})")
    print(f"    Timing: Sparse={metadata['sparse_time']:.2f}ms, Dense={metadata['dense_time']:.2f}ms, Total={metadata['total_time']:.2f}ms")

    print(f"\n    Top 3 Retrieved Documents:")
    for i, doc in enumerate(docs, 1):
        scores = metadata['scores']
        print(f"      {i}. {doc[:80]}")
        print(f"         BM25={scores['bm25'][i-1]:.3f}, Dense={scores['dense'][i-1]:.3f}, Hybrid={scores['hybrid'][i-1]:.3f}")

    print("-"*80)

print("\n Adaptive Hybrid Retrieval System working!")



print(" Building Comprehensive Evaluation Framework\n")

from sklearn.metrics import precision_score, recall_score, f1_score

class RAGEvaluator:
    """
    Comprehensive evaluation framework for RAG systems.
    Compares adaptive hybrid against static baselines.
    """

    def __init__(self, query_doc_pairs, corpus_texts):
        self.query_doc_pairs = query_doc_pairs
        self.corpus_texts = corpus_texts

    def evaluate_retrieval(self, retriever_func, queries, k=5, method_name=""):
        """
        Evaluate retrieval quality using multiple metrics

        Returns: metrics dictionary
        """
        print(f"\n{'='*70}")
        print(f" Evaluating: {method_name}")
        print(f"{'='*70}\n")

        metrics = {
            'precision_at_k': [],
            'recall_at_k': [],
            'mrr': [],  # Mean Reciprocal Rank
            'latency_ms': [],
            'complexity_distribution': {'SIMPLE': 0, 'MODERATE': 0, 'COMPLEX': 0}
        }

        num_queries = min(len(queries), 50)  # Evaluate on 50 queries max

        for i, pair in enumerate(queries[:num_queries]):
            query = pair['query']
            relevant_doc_id = pair['relevant_doc_id']

            # Retrieve documents
            start = time.time()
            if hasattr(retriever_func, 'retrieve'):
                # Adaptive retriever
                docs, metadata = retriever_func.retrieve(query, k=k)
                retrieved_indices = metadata['top_indices']
                complexity = metadata.get('complexity', 'UNKNOWN')
                metrics['complexity_distribution'][complexity] = \
                    metrics['complexity_distribution'].get(complexity, 0) + 1
            else:
                # Baseline retrievers (just return function result)
                retrieved_indices, docs = retriever_func(query, k)

            latency = (time.time() - start) * 1000
            metrics['latency_ms'].append(latency)

            # Calculate metrics
            relevant_retrieved = 1 if relevant_doc_id in retrieved_indices else 0

            # Precision@k and Recall@k (with single relevant doc)
            precision = relevant_retrieved / k
            recall = relevant_retrieved  # Binary: either found (1) or not (0)

            metrics['precision_at_k'].append(precision)
            metrics['recall_at_k'].append(recall)

            # MRR (Mean Reciprocal Rank)
            if relevant_doc_id in retrieved_indices:
                rank = list(retrieved_indices).index(relevant_doc_id) + 1
                mrr = 1.0 / rank
            else:
                mrr = 0.0
            metrics['mrr'].append(mrr)

            # Progress
            if (i + 1) % 10 == 0:
                print(f"   Processed {i+1}/{num_queries} queries")

        # Aggregate metrics
        results = {
            'Precision@5': np.mean(metrics['precision_at_k']),
            'Recall@5': np.mean(metrics['recall_at_k']),
            'F1@5': 2 * np.mean(metrics['precision_at_k']) * np.mean(metrics['recall_at_k']) /
                    (np.mean(metrics['precision_at_k']) + np.mean(metrics['recall_at_k']) + 1e-10),
            'MRR': np.mean(metrics['mrr']),
            'Avg_Latency_ms': np.mean(metrics['latency_ms']),
            'Std_Latency_ms': np.std(metrics['latency_ms']),
            'Num_Queries': num_queries
        }

        if 'complexity_distribution' in metrics and any(metrics['complexity_distribution'].values()):
            results['Complexity_Dist'] = metrics['complexity_distribution']

        return results

# Initialize evaluator
evaluator = RAGEvaluator(query_doc_pairs, corpus_texts_large)

print(" Evaluation framework ready!")
print(f" Will evaluate on {min(len(query_doc_pairs), 50)} queries")

print(" Running Complete Evaluation (This may take 2-3 minutes)\n")

# Define baseline retrievers
def dense_only_retriever(query, k=5):
    """Baseline: Dense retrieval only (BGE + FAISS)"""
    query_emb = model_bge.encode([query], convert_to_numpy=True)
    distances, indices = index_faiss_bge_large.search(query_emb, k)
    docs = [corpus_texts_large[idx] for idx in indices[0]]
    return indices[0], docs

def sparse_only_retriever(query, k=5):
    """Baseline: Sparse retrieval only (BM25)"""
    tokenized_query = tokenize_text(query)
    bm25_scores = bm25.get_scores(tokenized_query)
    top_indices = np.argsort(bm25_scores)[::-1][:k]
    docs = [corpus_texts_large[idx] for idx in top_indices]
    return top_indices, docs

def static_hybrid_retriever(query, k=5):
    """Baseline: Static hybrid (50-50 split, no adaptation)"""
    # BM25
    tokenized_query = tokenize_text(query)
    bm25_scores = bm25.get_scores(tokenized_query)
    bm25_scores_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)

    # Dense
    query_emb = model_bge.encode([query], convert_to_numpy=True)
    distances, _ = index_faiss_bge_large.search(query_emb, len(corpus_texts_large))
    dense_scores = 1 / (1 + distances[0])
    dense_scores_norm = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-10)

    # 50-50 hybrid
    hybrid_scores = 0.5 * bm25_scores_norm + 0.5 * dense_scores_norm
    top_indices = np.argsort(hybrid_scores)[::-1][:k]
    docs = [corpus_texts_large[idx] for idx in top_indices]
    return top_indices, docs

# Evaluate all methods
all_results = {}

print("="*80)
print("EVALUATION COMPARISON: ADAPTIVE vs BASELINES")
print("="*80)

# 1. Dense Only (Baseline)
results_dense = evaluator.evaluate_retrieval(
    dense_only_retriever,
    query_doc_pairs,
    k=5,
    method_name="Baseline: Dense Only (BGE + FAISS)"
)
all_results['Dense_Only'] = results_dense

# 2. Sparse Only (Baseline)
results_sparse = evaluator.evaluate_retrieval(
    sparse_only_retriever,
    query_doc_pairs,
    k=5,
    method_name="Baseline: Sparse Only (BM25)"
)
all_results['Sparse_Only'] = results_sparse

# 3. Static Hybrid (Baseline)
results_static = evaluator.evaluate_retrieval(
    static_hybrid_retriever,
    query_doc_pairs,
    k=5,
    method_name="Baseline: Static Hybrid (50-50)"
)
all_results['Static_Hybrid'] = results_static

# 4. Adaptive Hybrid (NOVEL METHOD)
results_adaptive = evaluator.evaluate_retrieval(
    adaptive_retriever,
    query_doc_pairs,
    k=5,
    method_name="NOVEL: Adaptive Hybrid (Query-Complexity-Aware)"
)
all_results['Adaptive_Hybrid'] = results_adaptive

print("\n" + "="*80)
print(" Evaluation Complete!")
print("="*80)

print("\n" + "="*80)
print(" FINAL RESULTS COMPARISON")
print("="*80 + "\n")

# Create results dataframe
results_df = pd.DataFrame({
    'Method': list(all_results.keys()),
    'Precision@5': [all_results[k]['Precision@5'] for k in all_results.keys()],
    'Recall@5': [all_results[k]['Recall@5'] for k in all_results.keys()],
    'F1@5': [all_results[k]['F1@5'] for k in all_results.keys()],
    'MRR': [all_results[k]['MRR'] for k in all_results.keys()],
    'Latency (ms)': [all_results[k]['Avg_Latency_ms'] for k in all_results.keys()],
    'Std (ms)': [all_results[k]['Std_Latency_ms'] for k in all_results.keys()]
})

print(results_df.to_string(index=False))

# Find best performers
print("\n" + "="*80)
print("BEST PERFORMERS")
print("="*80)

best_precision = results_df.loc[results_df['Precision@5'].idxmax()]
best_recall = results_df.loc[results_df['Recall@5'].idxmax()]
best_f1 = results_df.loc[results_df['F1@5'].idxmax()]
best_mrr = results_df.loc[results_df['MRR'].idxmax()]
fastest = results_df.loc[results_df['Latency (ms)'].idxmin()]

print(f"\n Best Precision@5: {best_precision['Method']} ({best_precision['Precision@5']:.4f})")
print(f" Best Recall@5: {best_recall['Method']} ({best_recall['Recall@5']:.4f})")
print(f" Best F1@5: {best_f1['Method']} ({best_f1['F1@5']:.4f})")
print(f" Best MRR: {best_mrr['Method']} ({best_mrr['MRR']:.4f})")
print(f" Fastest: {fastest['Method']} ({fastest['Latency (ms)']:.2f}ms)")

# Calculate improvements
if 'Adaptive_Hybrid' in all_results:
    print("\n" + "="*80)
    print("ADAPTIVE HYBRID IMPROVEMENTS OVER BASELINES")
    print("="*80)

    adaptive_f1 = all_results['Adaptive_Hybrid']['F1@5']
    adaptive_mrr = all_results['Adaptive_Hybrid']['MRR']
    adaptive_latency = all_results['Adaptive_Hybrid']['Avg_Latency_ms']

    for baseline in ['Dense_Only', 'Sparse_Only', 'Static_Hybrid']:
        if baseline in all_results:
            f1_improvement = ((adaptive_f1 - all_results[baseline]['F1@5']) / all_results[baseline]['F1@5']) * 100
            mrr_improvement = ((adaptive_mrr - all_results[baseline]['MRR']) / all_results[baseline]['MRR']) * 100
            latency_change = ((adaptive_latency - all_results[baseline]['Avg_Latency_ms']) / all_results[baseline]['Avg_Latency_ms']) * 100

            print(f"\nVs {baseline}:")
            print(f"   F1@5: {f1_improvement:+.1f}%")
            print(f"   MRR: {mrr_improvement:+.1f}%")
            print(f"   Latency: {latency_change:+.1f}%")

# Show complexity distribution for adaptive method
if 'Complexity_Dist' in all_results['Adaptive_Hybrid']:
    print("\n" + "="*80)
    print(" QUERY COMPLEXITY DISTRIBUTION (Adaptive Method)")
    print("="*80)
    dist = all_results['Adaptive_Hybrid']['Complexity_Dist']
    for complexity, count in dist.items():
        percentage = (count / all_results['Adaptive_Hybrid']['Num_Queries']) * 100
        print(f"   {complexity}: {count} queries ({percentage:.1f}%)")

print("\n" + "="*80)
print(" EVALUATION COMPLETE - READY FOR PUBLICATION!")
print("="*80)





print("\n Generating Performance Comparison Visualizations\n")

# ============================================================================
# FIGURE 2: Performance Metrics Comparison
# ============================================================================
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('RAG System Performance Comparison', fontsize=16, fontweight='bold')

# Prepare data
methods = list(all_results.keys())
method_labels = {
    'Dense_Only': 'Dense Only\n(BGE)',
    'Sparse_Only': 'Sparse Only\n(BM25)',
    'Static_Hybrid': 'Static Hybrid\n(50-50)',
    'Adaptive_Hybrid': 'Adaptive Hybrid\n'
}
methods_display = [method_labels.get(m, m) for m in methods]

precision_scores = [all_results[m]['Precision@5'] for m in methods]
recall_scores = [all_results[m]['Recall@5'] for m in methods]
f1_scores = [all_results[m]['F1@5'] for m in methods]
mrr_scores = [all_results[m]['MRR'] for m in methods]
latency_scores = [all_results[m]['Avg_Latency_ms'] for m in methods]

# 1. Precision, Recall, F1 Comparison
x = np.arange(len(methods))
width = 0.25

axes[0, 0].bar(x - width, precision_scores, width, label='Precision@5', color='#3498db')
axes[0, 0].bar(x, recall_scores, width, label='Recall@5', color='#e74c3c')
axes[0, 0].bar(x + width, f1_scores, width, label='F1@5', color='#2ecc71')
axes[0, 0].set_ylabel('Score', fontsize=11)
axes[0, 0].set_xlabel('Method', fontsize=11)
axes[0, 0].set_title('(a) Retrieval Quality Metrics', fontsize=12, fontweight='bold')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(methods_display, fontsize=9)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')
axes[0, 0].set_ylim([0, 1.1])

# 2. MRR Comparison
colors_mrr = ['#3498db' if m != 'Adaptive_Hybrid' else '#e74c3c' for m in methods]
bars = axes[0, 1].bar(methods_display, mrr_scores, color=colors_mrr, edgecolor='black', alpha=0.7)
axes[0, 1].set_ylabel('MRR Score', fontsize=11)
axes[0, 1].set_xlabel('Method', fontsize=11)
axes[0, 1].set_title('(b) Mean Reciprocal Rank (MRR)', fontsize=12, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3, axis='y')
axes[0, 1].set_ylim([0, 1.1])

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)

# 3. Latency Comparison (Log Scale)
colors_latency = ['#95a5a6' if m != 'Adaptive_Hybrid' else '#e74c3c' for m in methods]
bars = axes[1, 0].bar(methods_display, latency_scores, color=colors_latency, edgecolor='black', alpha=0.7)
axes[1, 0].set_ylabel('Latency (ms, log scale)', fontsize=11)
axes[1, 0].set_xlabel('Method', fontsize=11)
axes[1, 0].set_title('(c) Average Query Latency', fontsize=12, fontweight='bold')
axes[1, 0].set_yscale('log')
axes[1, 0].grid(True, alpha=0.3, axis='y')

# Add value labels
for bar in bars:
    height = bar.get_height()
    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.2f}', ha='center', va='bottom', fontsize=9)

# 4. Accuracy-Latency Trade-off (Scatter Plot)
axes[1, 1].scatter(latency_scores, mrr_scores, s=200, alpha=0.7,
                   c=['#3498db', '#95a5a6', '#f39c12', '#e74c3c'])

# Add labels for each point
for i, method in enumerate(methods_display):
    axes[1, 1].annotate(method, (latency_scores[i], mrr_scores[i]),
                       fontsize=9, ha='center', xytext=(0, 10),
                       textcoords='offset points')

axes[1, 1].set_xlabel('Latency (ms)', fontsize=11)
axes[1, 1].set_ylabel('MRR Score', fontsize=11)
axes[1, 1].set_title('(d) Accuracy-Latency Trade-off', fontsize=12, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)
axes[1, 1].set_xscale('log')

plt.tight_layout()
plt.savefig('/content/figures/fig2_performance_comparison.png', dpi=300, bbox_inches='tight')
print(" Figure 2: Performance Comparison saved to /content/figures/fig2_performance_comparison.png")
plt.show()









